logging:
  output_dir: ../output/results_mix_all_mathquiz
manager:
  name: rating_games
  game_pool_manager_args:
    max_matches: 50
  game_manager_args:
    max_rounds: 10
    max_player_attempts: 5
    win_conditions: 
      - StudentCorrect
      - StudentIncorrect
      - TeacherIncorrect
    draw_conditions:
      - Placeholder
game:
  name: mathquiz
  players:
    - name: mathquiz_teacher
      args:
        id: teacher
        roles: 
          - TeacherGenerateQuestion
          - TeacherAnswerQuestion
        max_tries: 1
    - name: mathquiz_student
      args:
        id: student
        roles: 
          - StudentAnswerQuestion
        max_tries: 1
llms:
  # - type: GoogleVertexAI
  #   name: gemini-1.5-pro
  #   args:
  #     model: publishers/google/models/gemini-1.5-pro-001
  #     max_tokens: 1600
  # - type: AzureOpenAI
  #   name: gpt-4o
  #   args:
  #     api_base: https://allam-swn-gpt-01.openai.azure.com/
  #     api_version: 2023-07-01-preview
  #     
  #     deployment_id: gpt-4o-900ptu
  #     max_tokens: 1600
  #     model_type: chat
  # - type: GROQ
  #   name: llama3.1-70b-default
  #   args:
  #     model: llama-3.1-70b-versatile
  #     max_tokens: 1600
  #     
  - type: OpenAI
    name: gpt-4o-default
    args:
      base_url: https://api.openai.com/v1/
      
      model: gpt-4o
      max_tokens: 1600
  - type: OpenAI
    name: llama3.1-70b-default
    args:
      
      model_type: chat
      base_url: http://localhost:8000/v1/
      model: meta-llama/Meta-Llama-3.1-70B-Instruct
      max_tokens: 1600
  - type: Mistral
    name: mistral-large-default
    args:
      model: mistral-large-latest
      
  - type: Claude
    name: claude-3-5-sonnet-default
    args:
      model: claude-3-5-sonnet-20240620
      
      max_tokens: 1600

  - type: OpenAI
    name: gpt-4o-optimized-mipro
    module_paths:
      White: compiled_modules/mipro/gpt-4o/white_prompts.json
      Black: compiled_modules/mipro/gpt-4o/black_prompts.json
    args:
      base_url: https://api.openai.com/v1/
      
      model: gpt-4o
      max_tokens: 1600
  - type: OpenAI
    name: llama3.1-70b-optimized-mipro
    module_paths:
      White: compiled_modules/mipro/llama3.1-70b/white_prompts.json
      Black: compiled_modules/mipro/llama3.1-70b/black_prompts.json
    args:
      
      model_type: chat
      base_url: http://localhost:8000/v1/
      model: meta-llama/Meta-Llama-3.1-70B-Instruct
  - type: Claude
    name: claude-3-5-sonnet-optimized-mipro
    module_paths:
      White: compiled_modules/mipro/claude-3-5-sonnet/white_prompts.json
      Black: compiled_modules/mipro/claude-3-5-sonnet/black_prompts.json
    args:
      model: claude-3-5-sonnet-20240620
      
      max_tokens: 1600
  - type: Mistral
    name: mistral-large-optimized-mipro
    module_paths:
      White: compiled_modules/mipro/mistral-large/white_prompts.json
      Black: compiled_modules/mipro/mistral-large/black_prompts.json
    args:
      model: mistral-large-latest
      
      max_tokens: 1600

  - type: OpenAI
    name: gpt-4o-optimized-bsfs
    module_paths:
      White: compiled_modules/bsfs/gpt-4o/white_prompts.json
      Black: compiled_modules/bsfs/gpt-4o/black_prompts.json
    args:
      base_url: https://api.openai.com/v1/
      
      model: gpt-4o
      max_tokens: 1600
  - type: OpenAI
    name: llama3.1-70b-optimized-bsfs
    module_paths:
      White: compiled_modules/bsfs/llama3.1-70b/white_prompts.json
      Black: compiled_modules/bsfs/llama3.1-70b/black_prompts.json
    args:
      
      model_type: chat
      base_url: http://localhost:8000/v1/
      model: meta-llama/Meta-Llama-3.1-70B-Instruct
  - type: Claude
    name: claude-3-5-sonnet-optimized-bsfs
    module_paths:
      White: compiled_modules/bsfs/claude-3-5-sonnet/white_prompts.json
      Black: compiled_modules/bsfs/claude-3-5-sonnet/black_prompts.json
    args:
      model: claude-3-5-sonnet-20240620
      
      max_tokens: 1600
  - type: Mistral
    name: mistral-large-optimized-bsfs
    module_paths:
      White: compiled_modules/bsfs/mistral-large/white_prompts.json
      Black: compiled_modules/bsfs/mistral-large/black_prompts.json
    args:
      model: mistral-large-latest
      
      max_tokens: 1600