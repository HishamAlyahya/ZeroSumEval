logging:
  output_dir: ../output/results_mix_all_chess
manager:
  name: rating_games
  game_pool_manager_args:
    max_matches: 100
  game_manager_args:
    max_rounds: 200
    max_player_attempts: 1
    win_conditions: 
      - Checkmate
      - Invalid
    draw_conditions:
      - Stalemate
      - Insufficient material
      - 75-move rule
      - Fivefold repetition
game:
  name: chess
  players:
    - name: chess_player
      args:
        id: white
        roles:
          - White
        max_tries: 5
    - name: chess_player
      args:
        id: black
        roles:
          - Black
        max_tries: 5
llms:
  # - type: GoogleVertexAI
  #   name: gemini-1.5-pro
  #   args:
  #     model: publishers/google/models/gemini-1.5-pro-001
  #     max_tokens: 1600
  # - type: AzureOpenAI
  #   name: gpt-4o
  #   args:
  #     api_base: https://allam-swn-gpt-01.openai.azure.com/
  #     api_version: 2023-07-01-preview
  #     
  #     deployment_id: gpt-4o-900ptu
  #     max_tokens: 1600
  #     model_type: chat
  # - type: GROQ
  #   name: llama3.1-70b-default
  #   args:
  #     model: llama-3.1-70b-versatile
  #     max_tokens: 1600
  #     
  - name: gpt-4o-default 
    model: openrouter/openai/chatgpt-4o-latest 

  - name: llama3.1-70b-default
    model: openrouter/meta-llama/Meta-Llama-3.1-70B-Instruct

  - name: mistral-large-default
    model: openrouter/mistral-large-latest
      
  - name: claude-3-5-sonnet-default
    model: openrouter/claude-3-5-sonnet-20240620
    
  # - type: OpenAI
  #   name: gpt-4o-optimized-mipro
  #   module_paths:
  #     White: compiled_modules/mipro/gpt-4o/white_prompts.json
  #     Black: compiled_modules/mipro/gpt-4o/black_prompts.json
  #   args:
  #     base_url: https://api.openai.com/v1/
      
  #     model: gpt-4o
  #     max_tokens: 1600
  # - type: OpenAI
  #   name: llama3.1-70b-optimized-mipro
  #   module_paths:
  #     White: compiled_modules/mipro/llama3.1-70b/white_prompts.json
  #     Black: compiled_modules/mipro/llama3.1-70b/black_prompts.json
  #   args:
      
  #     model_type: chat
  #     base_url: http://localhost:8000/v1/
  #     model: meta-llama/Meta-Llama-3.1-70B-Instruct
  # - type: Claude
  #   name: claude-3-5-sonnet-optimized-mipro
  #   module_paths:
  #     White: compiled_modules/mipro/claude-3-5-sonnet/white_prompts.json
  #     Black: compiled_modules/mipro/claude-3-5-sonnet/black_prompts.json
  #   args:
  #     model: claude-3-5-sonnet-20240620
      
  #     max_tokens: 1600
  # - type: Mistral
  #   name: mistral-large-optimized-mipro
  #   module_paths:
  #     White: compiled_modules/mipro/mistral-large/white_prompts.json
  #     Black: compiled_modules/mipro/mistral-large/black_prompts.json
  #   args:
  #     model: mistral-large-latest
      
  #     max_tokens: 1600

  # - type: OpenAI
  #   name: gpt-4o-optimized-bsfs
  #   module_paths:
  #     White: compiled_modules/bsfs/gpt-4o/white_prompts.json
  #     Black: compiled_modules/bsfs/gpt-4o/black_prompts.json
  #   args:
  #     base_url: https://api.openai.com/v1/
      
  #     model: gpt-4o
  #     max_tokens: 1600
  # - type: OpenAI
  #   name: llama3.1-70b-optimized-bsfs
  #   module_paths:
  #     White: compiled_modules/bsfs/llama3.1-70b/white_prompts.json
  #     Black: compiled_modules/bsfs/llama3.1-70b/black_prompts.json
  #   args:
      
  #     model_type: chat
  #     base_url: http://localhost:8000/v1/
  #     model: meta-llama/Meta-Llama-3.1-70B-Instruct
  # - type: Claude
  #   name: claude-3-5-sonnet-optimized-bsfs
  #   module_paths:
  #     White: compiled_modules/bsfs/claude-3-5-sonnet/white_prompts.json
  #     Black: compiled_modules/bsfs/claude-3-5-sonnet/black_prompts.json
  #   args:
  #     model: claude-3-5-sonnet-20240620
      
  #     max_tokens: 1600
  # - type: Mistral
  #   name: mistral-large-optimized-bsfs
  #   module_paths:
  #     White: compiled_modules/bsfs/mistral-large/white_prompts.json
  #     Black: compiled_modules/bsfs/mistral-large/black_prompts.json
  #   args:
  #     model: mistral-large-latest
      
  #     max_tokens: 1600